# Visualization Pipeline Automation - Complete Documentation Index

## ğŸ“‹ Quick Navigation

### For First-Time Users

1. Start here: [IMPLEMENTATION.md](IMPLEMENTATION.md) - 5-minute overview
2. Try it: `python run_visualization_pipeline.py --help`
3. Run it: `python run_visualization_pipeline.py /path/to/logs/journal.json`
4. Detailed help: [QUICK_START.sh](QUICK_START.sh)

### For Reference

- [PIPELINE_README.md](PIPELINE_README.md) - Complete documentation
- [TECHNICAL_REFERENCE.md](TECHNICAL_REFERENCE.md) - Implementation details
- [AUTOMATION_SUMMARY.md](AUTOMATION_SUMMARY.md) - Architecture overview

## ğŸ¯ What This Does

Automates the visualization pipeline:

```
journal.json â†’ [Judge] â†’ [Analyze] â†’ [Visualize] â†’ dashboard.html
                                  â†‘
                        Auto-config from metrics.csv
```

## ğŸš€ Quick Start (30 seconds)

```bash
cd visualization_final/

# Run with automatic metric configuration
python run_visualization_pipeline.py ~/path/to/competition-name-uuid/logs/journal.json

# That's it! The dashboard is ready.
```

## ğŸ“ Files in This Directory

### ğŸ¯ Core Pipeline Scripts

```
run_visualization_pipeline.py          â† Main orchestrator (607 lines)
                                         Runs 3-step pipeline: judge â†’ plan â†’ viz
                                         Auto-configures from metrics.csv
                                         Usage: python script.py /path/to/logs

journal_viz_.py                        â† Interactive HTML dashboard generator (480 lines)
                                         Creates tree visualization with metrics, diffs, LLM judgments
                                         Handles both 'parent' and 'parent_id' field formats
                                         Reads: journal_with_judgements.json, plan_redundancy_report.json

judge_journal.py                       â† LLM code verifier (150 lines)
                                         Uses Vertex AI to judge code correctness vs plan
                                         Compares child code to parent code
                                         Outputs: journal_with_judgements.json

plan_judge.py                          â† Plan redundancy analyzer (95 lines)
                                         Detects duplicate execution plans among sibling nodes
                                         Uses Gemini API for semantic similarity detection
                                         Outputs: plan_redundancy_report.json
```

### ğŸ”§ Batch Processing & Utilities

```
run_viz_only_all.sh                    â† Batch visualization regenerator (105 lines)
                                         Runs journal_viz_.py on all 169 runs
                                         Usage: ./script.sh --apply [--limit N]
                                         Skips judge/plan steps, only updates dashboards

run_viz_by_date.sh                     â† Date-based batch processor (120 lines)
                                         Processes runs grouped by date directory
                                         Supports --skip-judge for fast iteration
                                         Logs progress to run_viz_by_date.log

run_plan_judge_all.py                  â† Batch plan analyzer (243 lines)
                                         Runs plan_judge.py on all multi-child nodes
                                         Skips already-analyzed nodes (checks plan_redundancy_report.json)
                                         Handles 429 API errors with exponential backoff (2, 4, 8s)
                                         Usage: python script.py --apply [--limit N]

fix_judgment_errors.py                 â† LLM judgment error fixer (135 lines)
                                         Re-runs failed LLM judgments (status == "error")
                                         Exponential backoff on 429 RESOURCE_EXHAUSTED
                                         Max 3 retry attempts per entry

fix_all_judgment_errors.sh             â† Batch judgment error fixer (35 lines)
                                         Finds all journal_with_judgements.json with errors
                                         Calls fix_judgment_errors.py on each

update_tree_structure.py               â† Parent-child relationship reconstructor (229 lines)
                                         Rebuilds parent/children fields from node2parent mapping
                                         Handles both list and dict journal formats
                                         Usage: python script.py --apply [--limit N]
                                         Applied to 169 files: 29 updated, 1827 changes

standardize_journal_format.py          â† Journal format converter (116 lines)
                                         Converts {"nodes": [...]} to plain [...] format
                                         Ensures consistency across all journal files
                                         Usage: python script.py --apply [--limit N]
```

### ğŸ“Š Dashboard & Serving

```
generate_hyper_dashboard.py            â† Static hyper-dashboard generator (330 lines)
                                         Creates index HTML linking all run dashboards
                                         Groups by competition, shows metrics
                                         Outputs: hyper_dashboard.html

serve_hyper_dashboard.py               â† Dynamic dashboard HTTP server (400 lines)
                                         Live HTTP server on localhost:8000
                                         Auto-scans runs/ directory on each request
                                         No static file generation needed
                                         Usage: python script.py
```

### ğŸ§ª Testing & Debugging

```
test_pipeline.py                       â† Pipeline function tester (15 lines)
                                         Tests parse_journal_path() and load_metrics_from_csv()
                                         Uses real paths for validation

debug_plan_judge.py                    â† Plan judge debugger (147 lines)
                                         Shows nodes with multiple children
                                         Compares plan_redundancy_report.json before/after
                                         Real-time output streaming from plan_judge.py
```

### ğŸ“š Documentation

```
IMPLEMENTATION.md                      â† 5-minute project overview
PIPELINE_README.md                     â† Complete usage guide with examples
QUICK_START.sh                         â† Command reference with Q&A
TECHNICAL_REFERENCE.md                 â† Implementation deep dive
AUTOMATION_SUMMARY.md                  â† Architecture and design decisions
ARCHITECTURE_DIAGRAMS.md               â† Visual system architecture
COMPLETION_SUMMARY.md                  â† Project completion report
QUICK_REFERENCE.md                     â† Command cheat sheet
README_INDEX.md                        â† This file - navigation hub
```

### ğŸ“¦ Configuration & Data

```
metrics.csv                            â† Competition metrics database
                                         Fields: competition, metric_name, goal,
                                                ignore_buggy_without_metric, default_buggy_metric

config_0.7_20_3600.jsonl               â† LLM API configuration
experiments.csv                        â† Experiment tracking data
```

### ğŸ“ Generated Files (Created by Pipeline)

```
journal_with_judgements.json           â† Judge output (per run)
                                         Original journal + LLM verification

plan_redundancy_report.json            â† Plan analysis output (per run)
                                         Maps parent_id â†’ [[redundant child steps]]

journal_viz_tree_dashboard.html        â† Interactive dashboard (per run)
                                         Full tree visualization with metrics

hyper_dashboard.html                   â† Index of all run dashboards (top-level)
run_viz_only_all.log                   â† Batch processing log
```

## ğŸ“ Documentation Structure

```
IMPLEMENTATION.md
â”œâ”€ What You Asked For
â”œâ”€ What Was Built
â”œâ”€ How to Use
â”œâ”€ How It Works
â”œâ”€ Key Features
â”œâ”€ Requirements
â””â”€ Next Steps

PIPELINE_README.md
â”œâ”€ Overview
â”œâ”€ Installation
â”œâ”€ Usage (Basic & Advanced)
â”œâ”€ Metrics Configuration
â”œâ”€ Output Files
â”œâ”€ Troubleshooting
â””â”€ Notes

QUICK_START.sh
â”œâ”€ Quick Start Examples
â”œâ”€ Common Scenarios
â”œâ”€ Path Requirements
â”œâ”€ What It Does
â”œâ”€ Output Files
â”œâ”€ Troubleshooting Q&A
â”œâ”€ Environment Variables
â”œâ”€ Batch Processing
â””â”€ Success Indicators

TECHNICAL_REFERENCE.md
â”œâ”€ Core Functions Reference
â”œâ”€ Configuration Changes
â”œâ”€ Error Handling Strategy
â”œâ”€ Logging System
â”œâ”€ File I/O Operations
â”œâ”€ Key Design Decisions
â”œâ”€ Testing Checklist
â”œâ”€ Performance Characteristics
â”œâ”€ Compatibility
â””â”€ Future Enhancements
```

## âœ¨ Key Features at a Glance

| Feature                | Details                                                  |
| ---------------------- | -------------------------------------------------------- |
| **Auto Configuration** | Reads competition name from path, loads metrics from CSV |
| **One-Line Execution** | `python run_visualization_pipeline.py /path`             |
| **Smart Options**      | `--skip-judge`, `--skip-plan`, `--output`, `--metrics`   |
| **Error Recovery**     | Fails gracefully with helpful error messages             |
| **Color-Coded Output** | Easy to read terminal logging                            |
| **Zero Dependencies**  | Uses only Python standard library                        |
| **Idempotent**         | Safe to run multiple times                               |
| **Fast Iteration**     | Skip expensive steps with options                        |

## ğŸ“Š Workflow Overview

```
USER PROVIDES PATH
    â†“
PATH PARSING
    â”œâ”€ Extract: {competition_name}-{run_id}
    â””â”€ Find: journal.json
    â†“
METRIC LOADING
    â”œâ”€ Look up competition in metrics.csv
    â””â”€ Load: metric_name, goal, buggy_settings
    â†“
CONFIGURATION UPDATE
    â”œâ”€ Update METRIC_INFO in journal_viz_.py
    â”œâ”€ Update IGNORE_BUGGY_WITHOUT_METRIC
    â””â”€ Update DEFAULT_BUGGY_METRIC
    â†“
STEP 1: JUDGE
    â”œâ”€ Run judge_journal.py
    â””â”€ Output: journal_with_judgements.json
    â†“
STEP 2: ANALYZE
    â”œâ”€ Run plan_judge.py
    â””â”€ Output: plan_redundancy_report.json
    â†“
STEP 3: VISUALIZE
    â”œâ”€ Run journal_viz_.py
    â””â”€ Output: journal_viz_tree_dashboard.html
    â†“
SUCCESS
    â””â”€ Dashboard ready for viewing
```

## ğŸ”§ Usage Examples

### Most Common

```bash
# Full pipeline with auto-config (this is all you need)
python run_visualization_pipeline.py ~/data/my-competition-xyz/logs/journal.json
```

### With Options

```bash
# Fast iteration (skip LLM judgment)
python run_visualization_pipeline.py ~/data/my-comp-xyz/logs --skip-judge

# Custom output
python run_visualization_pipeline.py ~/data/my-comp-xyz/logs --output results.html

# Combine options
python run_visualization_pipeline.py ~/data/my-comp-xyz/logs --skip-judge --output final.html
```

### Batch Processing

```bash
# Process multiple experiments
for dir in ~/experiments/*/logs; do
    python run_visualization_pipeline.py "$dir"
done
```

## ğŸ“š Reading Order

**If you have 5 minutes:**

1. [IMPLEMENTATION.md](IMPLEMENTATION.md) - Overview

**If you have 15 minutes:**

1. [IMPLEMENTATION.md](IMPLEMENTATION.md)
2. [QUICK_START.sh](QUICK_START.sh)

**If you have 30 minutes:**

1. [IMPLEMENTATION.md](IMPLEMENTATION.md)
2. [PIPELINE_README.md](PIPELINE_README.md)
3. [QUICK_START.sh](QUICK_START.sh)

**If you want to understand everything:**

1. [IMPLEMENTATION.md](IMPLEMENTATION.md) - Overview
2. [PIPELINE_README.md](PIPELINE_README.md) - Complete guide
3. [TECHNICAL_REFERENCE.md](TECHNICAL_REFERENCE.md) - Deep dive
4. [AUTOMATION_SUMMARY.md](AUTOMATION_SUMMARY.md) - Architecture

## â“ FAQ

### Q: What's the path format?

A: `.../competition_name-run_id/logs/journal.json`

### Q: Do I need to edit journal*viz*.py manually?

A: No! The script does it automatically.

### Q: What if I don't have API keys?

A: Set `export GOOGLE_API_KEY="your_key"` or use OpenAI key.

### Q: Can I skip the judgment step?

A: Yes! Use `--skip-judge` flag.

### Q: How long does it take?

A: Depends on data size. Typically 5-30 minutes for full pipeline.

### Q: What if something breaks?

A: Check error messages (color-coded in terminal) or see Troubleshooting section.

### Q: Can I use a different metrics file?

A: Yes! Use `--metrics /path/to/metrics.csv`

### Q: Does it work on Windows?

A: Yes! Uses pathlib for cross-platform compatibility.

## ğŸš€ Next Steps

1. **Try it immediately**:

   ```bash
   cd visualization_final/
   python run_visualization_pipeline.py --help
   ```

2. **Read the docs**:

   ```bash
   cat IMPLEMENTATION.md  # 5-minute overview
   ```

3. **Run with your data**:

   ```bash
   python run_visualization_pipeline.py /path/to/your/logs/journal.json
   ```

4. **Check the output**:
   ```bash
   open journal_viz_tree_dashboard.html  # View in browser
   ```

## ğŸ“ Version Info

- **Created**: February 3, 2026
- **Status**: âœ… Production ready
- **Location**: `visualization_final/`
- **Tested with**: Python 3.7+, macOS, Linux

## ğŸ¤ Support

### Common Issues

**"Could not find journal.json"**

- Check path format: `{comp}-{id}/logs/journal.json`

**"Competition not found in metrics CSV"**

- Add entry to metrics.csv or check name matches

**"No API keys found"**

- Set: `export GOOGLE_API_KEY="your_key"`

### Debug Info

Run with full output:

```bash
python run_visualization_pipeline.py /path --skip-judge
```

Check what was parsed:

```bash
python -c "
from run_visualization_pipeline import parse_journal_path
comp, run_id, wd, jf = parse_journal_path('/your/path')
print(f'Competition: {comp}')
print(f'Working Dir: {wd}')
"
```

---

## Summary

You now have a complete, automated visualization pipeline that:

âœ… Takes a single path argument  
âœ… Auto-detects competition from path  
âœ… Loads metrics from CSV  
âœ… Configures visualization automatically  
âœ… Runs all 3 steps sequentially  
âœ… Produces an interactive dashboard

**No manual editing of code required!**

Start with [IMPLEMENTATION.md](IMPLEMENTATION.md) to learn more.
